{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 竞争性关键词推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "1. 从文档中读入搜索内容 --\n",
    "    从比赛数据的训练集中提取出搜索记录，并以utf-8格式保存，每条记录占一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从源数据中提取出搜索词条，并保存（格式：每行一个搜索词条）\n",
    "data = open(\"./user_tag_query.10W.TRAIN\",'r',encoding='gb18030')\n",
    "output_file = open(\"./raw_words.train\",'w',encoding=\"utf-8\")\n",
    "for line in data:\n",
    "    line_list = line.split('\\t')\n",
    "    line_list = line_list[4:]\n",
    "    output_line = \"\\n\".join(line_list)\n",
    "    output_file.write(output_line + '\\n')\n",
    "data.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps：搜狗的数据集内容有点黄呀～～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 分词\n",
    "    分词使用jieba分词工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "#加载数据\n",
    "train_data = open(\"./raw_words.train\",\"r\")\n",
    "#保存结果文件\n",
    "result_file = open(\"./cutted_words.train\",'w')\n",
    "\n",
    "for sentence in train_data:\n",
    "    sentence = sentence[:-1]\n",
    "    cut_word = jieba.cut(sentence)\n",
    "    line_string = \"\\t\".join(cut_word) + '\\n'\n",
    "    result_file.write(line_string)\n",
    "train_data.close()\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 数据清洗（去除停用词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "train_data = open(\"./cutted_words.train\",\"r\")\n",
    "#保存结果文件\n",
    "result_file = open(\"./linkfree_words.train\",'w')\n",
    "\n",
    "#去除链接类词条\n",
    "for line in train_data:\n",
    "    word_list = line.split('\\t')\n",
    "    if word_list[0] in ['http','https','ftp','http:']:\n",
    "        continue\n",
    "    line_string = \"\\t\".join(word_list) + '\\n'\n",
    "    result_file.write(line_string)\n",
    "train_data.close()\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在去除停用词之前，首先获取词频分布特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入数据，以一维列表形式\n",
    "def read_word_list(filename):\n",
    "    wordlist = [];\n",
    "    data_file = open(filename,'r')\n",
    "    for line in data_file:\n",
    "        line = line[:-1]\n",
    "        words = line.split('\\t')\n",
    "        wordlist.extend(words)\n",
    "    data_file.close()\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片 291072\n",
      "手机 191472\n",
      "多少 191411\n",
      "意思 190856\n",
      "小说 159799\n",
      "视频 157487\n",
      "2016 144241\n",
      "下载 134929\n",
      "大全 131463\n",
      "吃 128020\n"
     ]
    }
   ],
   "source": [
    "#统计词频并展示(需要空闲5G左右内存)\n",
    "from collections import Counter\n",
    "word_list = read_word_list('./cleandata_v3.train')\n",
    "count_result = Counter(word_list)\n",
    "for key, val in count_result.most_common(10):\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#持久化统计结果\n",
    "word_list = None\n",
    "result_file = open('./count_record_v3.txt','w')\n",
    "for key ,val in count_result.items():\n",
    "    result_file.write(key + \"\\t\" + str(val) + \"\\n\")\n",
    "result_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#停用词过滤器\n",
    "\n",
    "#\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\",\n",
    "#\"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\", \"你\",\n",
    "#\"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\"\n",
    "def stop_words_filter(word_list):\n",
    "    stop_word_list = set(['\\n','\\t','\\r',',','，',', ','之','可以','的','与','了', \"在\",'是',\"我\",\"和\",\"就\",\"不\",\"都\",\"一\", \"一个\",\"上\", \"也\", \"很\", \"到\", \"说\",\"要\", \"去\", \"你\",\"会\", \"着\", \"没有\", \"看\", \"好\",\"自己\", \"这\",'呢','怎样','如何','什么','',' ','怎么','吗','有'])\n",
    "    word_cleaned = []\n",
    "    for word in word_list:\n",
    "        if word not in stop_word_list:\n",
    "            word_cleaned.append(word)\n",
    "    return word_cleaned\n",
    "stop_words_filter([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "train_data = open(\"./cutted_words.train\",\"r\")\n",
    "#保存结果文件\n",
    "result_file = open(\"./cleandata_v3.train\",'w')\n",
    "\n",
    "#过滤分词结果中的停用词\n",
    "for line in train_data:\n",
    "    line = line[:-1]\n",
    "    word_list = line.split('\\t')\n",
    "    word_list = stop_words_filter(word_list)\n",
    "    if len(word_list) == 0:\n",
    "        continue\n",
    "    line_string = \"\\t\".join(word_list) + '\\n'\n",
    "    result_file.write(line_string)\n",
    "train_data.close()\n",
    "result_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "柔和\t双沟\t\n",
      "\n",
      "女生\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_file = open(\"./cleandata_v1.train\",'r')\n",
    "for i in range(10):\n",
    "    line = result_file.readline()\n",
    "    if (line == \"\\n\"):\n",
    "        continue\n",
    "    print(line)\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(\"./raw_words.train\",\"r\")\n",
    "line = train_data.readline()\n",
    "train_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "柔和双沟\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba as jieba\n",
    "line = \"柔和双沟\\n\"\n",
    "# line = line[:-1]\n",
    "words = jieba.cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'柔和\\t双沟\\t\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\t\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x7f813df26138>\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
